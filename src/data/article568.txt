Автор: Blacksorld
Дата: 2021-09-28T07:15:01.000Z
Название: О чём не пишут в научных статьях или что на самом деле выделяет ваш
маттинг?
Теги: Блог компании Prisma Labs, Обработка изображений *, Машинное обучение
*, Искусственный интеллект, IT-компании
Текст:
Современные фоторедакторы используют нейросетевые модели
маттинга/сегментации для реализации большой доли своих инструментов,
таких как замена фона и боке. Приложение Lensa не стала исключением из
этого правила. Но правда ли, что информации из научных статей
достаточно для объяснения поведения обученных моделей для ваших
приложений? В какой-то момент у нас появилась идея: “А почему бы не
использовать маску маттинга для того, чтобы понимать, есть ли человек
на фото или нет?” На первый взгляд, задумка кажется весьма простой в
реализации, ведь раз модель умеет выделять людей, то определить по
предсказанной маске, есть ли человек на фото, не должно составить
труда. На практике мы столкнулись с довольно неожиданным эффектом.
Оказалось, что сетка, обученная только на людях, также выделяет
животных, статуи, еду и другие объекты. Меня зовут Дима, работаю в R&D
отделе Prisma Labs и в этом посте хочу рассказать, чем обусловлен
такой эффект и ещё как его можно использовать у себя в
приложении.Сначала коротко и наглядно об эффекте, который стал
причиной для постаЕсли вы не знакомы с задачей маттинга, про неё можно
узнать из несколько первых параграфов этой статьи. Спойлер, для
понимания сути поста не требуется глубокого понимания задачи. Каждая
из рассматриваемых далее моделей сегментации/маттинга предсказывает на
выходе некоторую одноканальную маску (бинарную или непрерывную). Этих
вводных достаточно.Итак, чтобы поскорее ввести вас в курс дела
исследуемого эффекта, начнём с парочки примеров. Ниже представлены
результаты работы одной из наших обученных моделей маттинга людей
(сразу с применением к замене фона):Слева направо: оригинал /
предсказанная маска / замена фона по этой маскеЕще несколько примеров
работы нашего маттинга на фотографиях без людейВ отрыве от контекста
обучения, примеры, на первый взгляд, не представляют ничего
интересного – модель выделяет разные объекты с переднего плана c
весьма средним для данной задачи качеством. Результаты становятся на
порядок значимее в совокупности с фактом, что в обучающем датасете
были только фотографии с разметкой людей (никаких животных, статуй,
еды и прочего).  Получается, что модель, которая обучалась только на
фотографиях с людьми, почему-то научилась неплохо выделять многие
другие объекты. Забавно, не правда ли?Есть ли такой же эффект в open-
source решениях?Ситуация, когда нейросеть для сегментации или
маттинга, обучаясь только на картинках с людьми, выделяет некоторые
другие объекты, не такая уж и редкая, на самом деле. Проблема
присутствует во многих open-source решениях и даже в недавно
появившихся SOTA подходах как, например, ModNet. Из самой статьи вы,
конечно же, про это ничего не узнаете, но проверить присутствие
эффекта можно в авторской веб-демке. Скрин примера работы модели
ModNet в авторской веб-демкеПри этом некоторые open-source модели
выделяют исключительно людей (people-segmentation). Давайте попробуем
разобраться, с чем может быть связано такое поведение.Про наши данные
и разметкуПричиной появления эффекта выделения других объектов вполне
могут стать особенности обучающего датасета.Для обучения модели
маттинга человека мы собрали собственный датасет. Требование к
разметке было обусловлено исключительно прикладным применением
маттинга. Это означает, что мы не опирались на разметку каких-либо
open-source датасетов, а думали только о том, как будет выглядеть
результат инструментов (замена и размытие фона), если тот или иной
предмет со сцены попадёт в итоговую маску. Вот несколько ключевых
фактов, которые характеризуют наш датасет:Практически все сэмплы в
датасете – портретные фото людей или селфи.На каждой фотографии из
датасета присутствует как минимум один человек. Не все люди с
фотографии попадают в маску.В маску вместе с человеком могут попадать
предметы, с которыми этот человек непосредственно взаимодействует:
аксессуары, животные, еда. Подобные предметы мы называем “interacted
objects”.Чем обоснованы такие требования?Практически все сэмплы в
датасете – портретные фото или селфи людейЕщё перед созданием
приложения Lensa было проведено исследование, которое показало, что
чаще всего пользователи Prisma загружают селфи и портретные фото. По
этой причине Lensa позиционировалась как редактор для фотографий
людей, и весь AI был направлен на обработку таких кейсов.На каждой
фотографии из датасета присутствует как минимум один человекТак как
изначально все фичи в приложении были задуманы исключительно для
фотографий с людьми, выделять какие-то другие объекты, кроме людей, не
было смысла. Здесь важно отметить, что в базовом эксперименте наша
модель не видела изображений без людей вообще (с нулевой маской). О
том, что будет, если добавить такие фотографии в датасет, я тоже
обязательно расскажу.Не все люди на фотографии попадают в маскуЛюди с
заднего плана не должны оставаться на фотографии при замене фона.
Формально определить передний и задний план, а тем более понять по
фотографии к какому из них относится конкретный человек, достаточно
сложно. Если попробовать описать одним предложением все наши пункты
тз, связанные с данным моментом, то звучать он будет примерно так:
“Если человек попал в кадр неслучайно, он должен быть в маске”.Пункт
из тз: люди с заднего планаЕсли на переднем плане несколько людей, то
всех надо выделять. Если есть люди на заднем плане, то они не должны
выделяться. Человек должен быть выделен, если он в этом кадре явно
неслучайно, или он явно на переднем плане.Ниже представлена парочка
наглядных примеров, чтобы лучше понять этот критерий.Первый пример –
люди с заднего плана не хотели попадать в кадр; второй пример – все
люди на фотографии намерненно попали в кадрНа первой фотографии два
человека за столом попали в кадр случайно, замена фона будет выглядеть
лучше, если они не будут присутствовать в финальной маске. Во втором
примере все люди на фотографии намеренно попали в кадр, поэтому они
должны быть в маске.Конечно, наше условие тоже не универсально и
возможны неочевидные случаи. Например, непонятно какой критерий лучше:
человек попадает в маску, если он сам хотел попасть на фотографию, или
же, если фотограф посчитал нужным, чтобы этот человек оказался в
объективе. Для большинства кейсов эти два критерия дают одинаковый
результат, но бывают и спорные моменты.Interacted objectsЭто,
наверное, самый сложный и неоднозначный пункт во всём тз. Мы до сих
пор находим много примеров, которые либо не удовлетворяют тз, либо
показывают, что его стоит доработать. Что же такое “interacted
objects” и почему какие-то предметы кроме человека должны попадать в
маску? Interacted objects – это предметы, с которыми люди с переднего
плана непосредственно взаимодействуют. Отношение “взаимодействует”
может означать: держит в руке / носит на голове / носит на руке … Чаще
всего это предметы гардероба, еда, разные аксессуары, животные. Если
удалить такой предмет из маски, то замена фона или любая другая фича,
опирающаяся на сегментацию, будет выглядеть странно. Ниже приведено
несколько примеров с interacted objects с правильной разметкой и
заменой фона:Примеры с interacted objectsЕсли на приведенных
фотографиях из маски удалить interacted objects, то замена фона будет
выглядеть весьма странно; в человеке, например, могут появиться дырки.
Если удалить из руки предмет, который даже не перекрывает силуэт
человека, то результат вставки также будет выглядеть нереалистично,
поскольку рука человека будет в неестественном положении.Больше
примеров с interacted objectsСейчас наше тз не идеально и возникает
множество неоднозначных ситуаций. Ниже приведено несколько примеров,
которые размечены по нашему тз и являются весьма спорными:Спорные
примеры разметки, которые удовлетворяют нашему тзНа первой фотографии
ноутбук можно считать за interacted objects, но в нашу маску разметки
он не попадёт, так как касание чаще всего не считается в нашем тз
взаимодействием (касание считается взаимодействием только для
некоторых категорий, например, животных). Аналогичная ситуация с
деревом на второй фотографии – предметы, на которые опирается человек:
столбы, деревья, стулья не попадают в маску. Получается, что отношение
“взаимодействие” в нашем тз ещё зависит от самого объекта
взаимодействия; конь или мотоцикл, на котором сидит человек, попадают
в маску, а стул – нет. Более того, лампа на первой фотографии попадает
в маску только частично (часть, перекрывающая человека), что тоже при
замене фона будет выглядеть так себе.  Из примеров видно, что в нашем
тз есть множество спорных кейсов. Заранее предусмотреть все случаи
разметки практически невозможно. Улучшение нашего датасета (правил его
разметки) весьма актуальная и важная задача.ЭкспериментыПеред тем, как
раскрыть вам все карты и показать, что же на самом деле является
причиной появления исследуемого эффекта, хотелось бы рассказать в двух
словах про наш пайплайн обучения. Мы обучаем маттинг в две стадии.
Первая состоит в том, чтобы выучить модель портретной сегментации в
низком разрешении на грубой разметке на весьма большом датасете. По
сути, это стандартная сегментация на два класса: фон и человек с
форграунда. Во время второй стадии мы обучаем модель маттинга в
высоком разрешении на более точной разметке, но при этом на меньшем
количестве размеченных фото, подавая на вход предсказание портретной
сегментации с первой стадии. Модели учатся на разные лоссы, потому что
маттинг решает уже не задачу двухклассовой сегментации (подробнее про
задачу маттинга можно узнать, например, из этой статьи). Такой
пайплайн приводит к резонному вопросу, в какой момент возникает
исследуемый эффект. Появляется ли он во время обучения портретной
сегментации или же на второй стадии – у модели маттинга. Ниже
представлены предсказания двух моделей из одного эксперимента
обучения: портретная сегментация и маттинг, в обучении которого
участвовала эта же портретная модель.Примеры работы моделей на
фотографиях без людей. Вторая строчка – результаты портретной
сегментации; третья строчка – предсказания маттинга.  Примеры выше
показывают, что главной причиной выделения какого-либо форграунда,
кроме человека, является портретная сегментация, зачастую маттинг лишь
уточняет выделенный объект. Бывают случаи, где портретная модель
выделила объект, но при этом маттинг его “удалил” (самый правый пример
на коллаже). Такие ситуации могут возникать из-за того, что во время
обучения маттинга мы также дообучаем и портретную сегментацию.
Примеров, где модель сегментации ничего не выделила, но при этом
маттинг определил передний план, мы не обнаружили. Самый важный вывод,
который нужно сделать из приведенных примеров: появление исследуемого
эффекта обусловлено именно обучением модели портретной сегментации. В
связи с этим во всех наших экспериментах речь пойдёт именно про
влияние того или иного фактора на её поведение.Обучение на open-source
датасетахМы обучили нашу портретную сегментацию на двух популярных
датасетах: Supervise и AISegment. Magic-эффект (выделение переднего
плана) при этом остался, так что можно смело ставить знак равно между
нашим датасетом и указанными open-source вариантами для исследования
эффекта выделения форграунда. Все отличия в разметке нашего датасета и
Supervise&AISegment на него никак не повлияли.Убираем interacted
objectsКак в наших, так и в open-source датасетах interacted objects
довольно часто попадали в маску. Мы предположили, что это не позволяет
сети выучить чёткое разграничение между людьми и другими объектами
переднего плана. Другими словами, если модели подать на вход
фотографию, где нет человека, но есть объект, который она видела в
обучении в качестве interacted object, она его также выделит.Датасет
Multi-Human Parsing v2.0 содержит маски из 58 категорий (очки, левый
ботинок, правая рука, сумка, и т.д.) для каждого человека на
изображении. С помощью такой разметки можно контролировать, какие
именно объекты должны попадать в разметку.В первом эксперименте мы
включили в маску все 58 категорий, но удалили из датасета сэмплы, на
которых присутствуют категории, входящие в interacted objects (мячи,
зонтики, и т.д.).Во втором эксперименте использовались все сэмплы,
однако категории interacted objects не попадали в маску.Примеры
разметки MHP для указанных экспериментовНа картинках ниже слева
направо представлены: оригинальное изображение, оригинальная
многоклассовая маска, бинарная маска с исключенными interacted. Все
представленные сэмплы были удалены из датасета в первом
эксперименте.Слева направо: оригинальное изображение, оригинальная
многоклассовая маска, бинарная маска с исключенными interacted
objectsВ обоих случаях, обученная сеть всё ещё выделяла различные
объекты на фото без людей. Добавляем картинки без людейЭксперименты с
разметкой масок не дали никакого результата, поэтому логично
предположить, что проблема кроется в низком разнообразии самих
фотографий в обучающем датасете. Так как все сэмплы в нашем датасете –
фото людей, самым логичным способом увеличения разнообразия будет
добавление сэмплов без людей с нулевой gt маской. Такие примеры мы
называем “negative samples”.Мы провели два эксперимента, в первом из
них каждое четвёртое изображение мы брали из датасета ADE20K, во
втором – из датасета COCO2017 (только те фото, в категориях которых
нет человека).Добавление изображений из датасета ADE20K помогло лишь
немного ослабить эффект. Сеть перестала выделять объекты на
фотографиях пейзажей, городских улиц и т.п., но, например, животные и
еда всё так же часто оказывались в маске. Это связано с тем, что в
ADE20K практически на всех фотографиях нет явного переднего плана. В
основном на картинках присутствуют пейзажи, улицы, интерьеры разных
комнат и т.д. Датасет COCO содержит более 55k картинок, включающий 170
различных категорий объектов, не считая человека. Добавление таких
сэмплов в обучающую выборку помогло практически полностью победить
эффект.Ниже представлено несколько примеров сравнения обученных
моделей:Слева направо: оригинальное изображение, сегментация без
negative samples, сегментация с добавлением negative samples из
ADE20K, сегментация с добавлением negative samples из COCOЕще больше
примеров сравненияВывод: если вы хотите обучить модель, которой нужно
выделять только людей, то необходимо добавить в датасет разнообразные
примеры с другими форграундами и нулевой маской. Важно, чтобы negative
samples покрывали как можно больше категорий объектов (именно на
переднем плане). Недостаточно просто добавить фото с пейзажами. При
этом на датасете только с людьми (без negative samples) можно обучить
модель, которая неплохо сможет выделять и другие объекты вроде
животных, еды, статуй.Экспериментальная часть на этом закончена. Этот
пост – не научная статья, а скорее тру-стори. В оставшейся части поста
мы расскажем, какое применение нашей модели с выделением форграундов
нашли у себя в приложении. Как говорится, не баг, а фича: как мы
используем неожиданный эффект нашей модели в приложенииВ нашем
приложении большинство фич долгое время были доступны только для
изображений, на которых есть лицо человека. Например, раньше, если вы
хотели обработать фотографию, на которой человек стоит спиной, многие
основные инструменты, такие как замена фона и боке были недоступны. Не
так давно мы задались вопросом, как же сделать доступными эти фичи для
большего количества категорий изображений. Помимо людей спиной/боком
хотелось бы также применять многие инструменты для фото с животными
или едой, посколько они также являются весьма востребованными для
многих пользователей. Очевидная идея – обучить отдельную модель
сегментации основного объекта (форграунда) на сцене для general case
фотографий. Это достаточно непросто, особенно учитывая тот факт, что у
нас нет собственного большого датасета для такой задачи, и вдобавок
ещё одна сетка увеличит размер нашего приложения и сделает более
ресурсозатратным общий пайплайн обработки. Нашему удивлению не было
предела, когда мы узнали, что для многих фотографий, благодаря
вышеописанному эффекту модели маттинга (на тот момент мы про него не
знали), даже не придется ничего обучать. Далее я расскажу, как мы
используем маттинг в инструментах для general case фотографий. Замена
фонаНа первый взгляд, кажется что здесь всё просто. Модель маттинга
предсказывает маску, по которой мы просто делаем замену фона. Ниже
несколько неплохих примеров. Первая проблема состоит в том, что для
хорошей замены фона на фотографии должен быть явный форграунд. Ниже
представлены примеры предсказаний модели на изображениях, где нет
чёткого переднего плана.Несколько примеров, на которых нет четкого
переднего плана, а также результаты нашего маттингаНа приведённых
примерах модель ничего толком не выделила, так как явного переднего
плана на фотографии нет. Замена фона на таких кейсах будет выглядеть
как минимум странно. Второй проблемный кейс – на фотографии есть
форграунд, но наша модель его либо совсем не нашла, либо недостаточно
хорошо выделила. Так как модель обучалась только на фотографиях с
людьми, нет никакой гарантии, что наша модель способна хорошо выделять
все объекты (категории). Вполне реальна ситуация, что на одном фото с
животным наша модель идеально выделит нужный форграунд, а на другой
качество сегментации будет совсем низким (модель выбирается только по
метрике качества выделения людей). Более того, даже если бы мы знали
про нашу модель, что она хорошо работает, например, на собаках и плохо
на жирафах, в приложении отсекать жирафов мы бы не могли (для этого
нужен опять отдельный классификатор). Ниже приведено несколько
примеров, на которых наша модель недостаточно хорошо выделила
форграунд для замены фона:На каждой фотографии есть четкий передний
план, но наша модель плохо его выделилаНа приведённых выше фотографиях
нет смысла давать пользователю менять фон, так как результат будет
выглядеть несуразно. Для борьбы с фильтрацией нежелательных примеров
мы придумали эвристику, которая позволяет нам отсеивать фотографии без
форграунда и примеры, где передний план выделен неуверенно. Просмотрев
тысячи примеров работы модели, мы пришли к заключению, что чаще всего
замена фона работает хорошо, если большая часть выделенного форграунда
(ненулевые значения в предсказанной маске) имеет значение
предсказанного альфа близкое к единице. На этом факте и основана наша
эвристика. С такой фильтрацией мы можем делать замену фона на многих
фотографиях животных, еды, статуй и других примерах с весьма неплохим
качеством. Ну, а если нет гарантии высокого качества результата, то и
пробовать незачем. Не стоит лишний раз пугать пользователя плохим
качеством фичей. Польза от фильтрации фотографий сразу станет видна на
парочке примеров, которые не проходят нашу эвристику:Примеры, которые
не проходят эвристику. Замену фона на таких примерах делать не
стоитБокеДля честного боке эффекта необходимо знать не только
расстояние до всех объектов на изображении. Если углубиться в детали,
для правильного боке вам необходимо: Оценить расстояние до каждого
объекта;Сфокусироваться на какой-то главный объект;Оценить, насколько
далеко каждый объект находится от области, которая должна попасть в
фокус и в зависимости от этого наложить на него сильный или слабый
блюр.После того как мы обучили модель, которая оценивает расстояние до
каждого объекта (глубину), камнем преткновения для этой фичи стало
также определение форграунда. Всегда можно попросить пользователя
тыкнуть на объект на изображении, который должен попасть в фокус (не
должен блюрится). Но, так как нашей главной фишкой приложения является
улучшение качества фотки без лишних вовлечений юзера, хотелось бы
автоматически определять, на какую часть изображения хотел бы
сфокусироваться пользователь. Для фотографий, на которых есть лица
людей, мы можем сфокусироваться на одно из лиц крупным планом (так
сделано в портретном режиме айфонов). А что делать с фотками, на
которых нет лиц? Здесь на помощь опять спешит наш магический эффект
сетки маттинга с определением форграунда в кадре. Идея очень простая,
давайте спросим у модели маттинга, что на фотографии является передним
планом (получим маску) и сфокусируемся на объект/объекты, которые
попадают в эту маску. Ниже можно увидеть несколько примеров
фокусировки по маске маттинга:Примеры фокусировки по маске маттинга в
бокеЕсли наша модель маттинга плохо выделила передний план, то
автоматическая фокусировка также будет плохой. По этой причине для
боке мы используем ту же эвристику, что и в замене фона, чтобы
отфильтровать фотографии, на которых мы не можем найти фокус по
предсказанию маттинга. На самом деле, для правильного поиска фокуса
точность маски маттинга может быть ниже, чем для замены фона. Нужно
узнать только примерное расстояние фокусировки, поэтому в боке
параметры эвристики более лояльны и пропускают заметно больше
фотографий. В итоге на некоторых изображениях качество боке может быть
высоким при низком качестве замены фона.Другие фичиСейчас мы даём
пользователю возможность изменять экспозицию, контрастность и тени
отдельно для переднего и для заднего плана. Раньше такая возможность
была доступна только для фотографий, где есть лицо человека. Логика
использования маски форграунда там практически такая же, как и в
замене фона, поэтому рассказывать про применение исследуемого эффекта
в этих фичах нет особого смысла, но набор инструментов, где мы
используем необычный эффект нашего маттинга, не ограничивается заменой
фона и боке.Итак, послесловиеВ этом посте я рассказал вам про весьма
неожиданный для нас эффект обучения моделей в задачах маттинга и
портретной сегментации. Наша команда занимается задачей
маттинга/портретной сегментации уже более четырех лет, при этом
обнаружила описанный в посте эффект менее года назад. Как и все
продукт-ориентированные рисёрчеры мы придумали, как использовать у
себя в приложении такую особенность модели, а только потом выяснили
причины полученного эффекта. Я не видел ни одной статьи, где авторы
писали про такой эффект и борьбу с ним (будет круто, если поделитесь),
при том, что многие модели, так же как и наша, подвержены выделению
форграунда. Вероятно, подобный эффект может появиться и в других
задачах CV, например, в детекции. Поэтому мы и решили поделиться с
вами нашими исследованиями по данному вопросу. Надеемся, что было
интересно и полезно. Рекомендуем вам не забывать про negative part
составляющую в обучении. Сетки, также как и люди, учатся не только на
положительном опыте, но и на негативных (отрицательных) примерах. Мы
ни в коем случае не говорим, что задачу выделения переднего плана надо
решать через обучение сегментации на датасете с людьми (особенно если
у вас есть разметка и для других категорий). В этом посте я лишь
рассказал про замеченный нашей командой эффект и про причины его
появления. Эффект был использован в фичах скорее из-за недостатка
обучающих данных для general case и ресурсных ограничений нашего
приложения. Наша история показывает, что иногда DL творит чудеса и
делает то, о чем вы его не просили. В нашем случае это чудо помогло
сделать много новых прикольных фич просто “за вжух” :) Всем удачных
negative samples и позитивного настроения. До встречи!
