Автор: ItisMarketing
Дата: 2021-10-06T14:38:47.000Z
Название: Многомерные данные и оценка качества их визуализации
Теги: Промышленное программирование *, Визуальное программирование *
Текст:
Многомерные данные — что они из себя представляют?  Зачем их
визуализировать и что мы можем понять из визуализации? Какими
способами можно уменьшить размерность таким образом, чтобы сохранилась
главная структура данных и какие свойства учитывать при
проектировании?  На Innopolis ML Meetup разбирали ответы на вопросы
вместе с Елизаветой Батаниной ML Engineer в компании Provectus. До
старта работы изучала Data Science, завершила курсы по Machine
Learning, Deep Learning и Computer Vision.   Итак, начнем с простого:
What is Data?  В машинном обучении, данные представлены как массивы
чисел, например: пиксели в изображениях. Удобно представлять эти
данные не как массивы чисел, а как векторы в многомерном пространстве.
Тогда, датасет — это облако точек в пространстве большой размерности.
Но, к сожалению, в 99,99% задач машинного обучения, у нас размерность
гораздо больше, чем 3. Обычно, это несколько сотен. Конечно,
представить себе пространство размером 512 будет очень сложно для
человека. Поэтому, конечно хочется уменьшить эту размерность. Но,
прежде чем это сделать, давайте разберемся, зачем вообще нужно
визуализировать данные?    Why visualize?   Визуализация — это часть
исследования тренировочных данных. Таким образом, мы можем понять,
какие классы у нас лучше всего разделены. Например, если какие-то
сэмплы плохо размечены, неправильно размечены, или же их лучше убрать
из тренировки.   Полезно визуализировать данные не только
тренировочные, но и продакшн. Предположим, если у нас продакшн данные
немного смещаются, происходит data drift, тогда, границы нашей модели,
которые разделяют классы, будет уже неверна и предсказания мы будем
получать уже совсем не те. Это актуально для сезонных данных.
Следующий аспект, для чего нужно визуализировать данные — найти
outliers. Например, если в backend что-то пошло не так, и в модель
классификации отзывов у вас приходят не отзывы клиентов, а слова из
товарных чеков для совершенно другой модели. Вроде бы, это тоже слова,
но домен уже другой, и какие-то статистические метрики могут не
обнаружить разницу.  С мотивацией разобрались, теперь встает вопрос:
как уменьшить размерность с нескольких сотен до 2D или 3D, таким
образом, чтобы сохранилась главная структура наших данных?
Dimensionality reduction   Конечно, ответ на этот вопрос уже есть,
существует много алгоритмов, в том числе линейные. Однако, для данных
очень большой размерности, линейные алгоритмы не работают, так как
очень сложно интерпретировать разные измерения. Поэтому, используют
нелинейные алгоритмы понижения размерности или алгоритмы для обучения
многообразием (Manifold Learning) К ним относятся MDC, t-SNE,
LargeVis, UMAP, TRIMAP и другие.   В этом многообразии алгоритмов, их
гипер параметров понятно, что проекция данных на меньшей размерности
будет совершенно разная и она нам будет говорить совершенно разную
историю. Хороший пример: проектирование нашей земли на плоскость.
Карта, которую все мы знаем по урокам географии в школе, очень сильно
искажается на краях, и в действительности, Россия не такая уж большая,
как нам говорили учителя.   Как этого избежать? Ответ прост — нужно
ввести какую-то метрику, которая будет показывать, насколько верна
наша проекция данных в меньшей плоскости. И на основе этой метрике, мы
сможем подобрать гипер-параметры, которые нам нужны.   Projection
quality metrics     Прежде чем выбрать метрику, необходимо понять,
какие свойства и факторы проекции нам нужны.     Мы бы хотели, чтобы
наша визуализация или проекция, отображала глобальную структуру как
можно четче. Чтобы оценить глобальную структуру, была придумана такая
метрика, как Global Score.      Авторы утверждают, что можно взять
методы главных компонентов PCA, как идеальный метод по сохранению
глобальной структуры, так как кроме глобальной структуры, PCA ничего и
не использует.      Если взять PCA, как эталон, то мы сможем сравнить
с ним все остальные алгоритмы. Как мы это сделаем? Просто возьмем и
посчитаем ошибку восстановления из меньшей размерности в большую
размерность для наших данных. И посчитаем ту же самую ошибку для
данных, которые мы уже понизили с помощью метода главных компонентов.
На основе этого, посчитаем Global Score, где максимум будет у данных
пониженной размерности методом главных компонентов, а все остальные
методы будут иметь немного меньше.     В данном случае, учитывается
взаимосвязь точек внутри маленьких соседств. Что для этого
использовать? Самый прямолинейный метод — взять и прогнать KNN на
наших данных с пониженной размерностью и выбрать тот метод, который
имеет наибольший тест accuracy. Но, проблема данного способа
заключается в том, что это supervised, мы должны иметь доступ до
лейблов наших данных. А также, KNN будет давать больше привилегий той
визуализации, которая разделяет кластеры, чем больше, тем лучше.
Можно попробовать другой прямолинейный метод — посчитать ошибку
Sammon’s error. Идея очень простая, мы считаем для каждой точки
дистанцию до любой другой точки в оригинальном и пониженном
пространстве и считаем разницу между этими дистанциями. В пониженном
пространстве, дистанция до всех точек была такая же, как в оригинале.
Проблема этого метода заключается в том, что он не учитывает проклятие
размерности: в большой размерности, дистанция между каждыми точками
гораздо больше, чем в пониженной размерности.   Существует и
дополнительный метод, по которому мы должны учитывать не дистанцию,
как число, а ранк, то есть порядок точек по дистанции до определенно
выбранного сэмпла. Мы хотим, чтобы при понижении размерности, вокруг
конкретной точки ее соседство было точно таким же, как и в
оригинальном пространстве. При этом, мы наказываем точки, которые
подходят слишком близко к локальному соседству, в которое они не
должны входить.    Для этого, мы вводим такую метрику, как надежность
(Trustworthiness) Она гарантирует нам при выборе любого сэмпла, что
все ближайшие точки будут ближайшими и в оригинальном пространстве.
Еще одна метрика — прерывность (Discontinuity), которая гарантирует
то, что точки из локального соседства не будут уходить слишком далеко,
в какой-то совершенно другой край визуализации.      Данная метрика
интересна тем, что она работает на глобальной структуре и локальной
структуре. Метрика используется для сравнения многообразий между
собой, а также, для сравнения тех же многообразий с отличающейся
размерностью (в оригинальной и уменьшенной)     Стабильность — тот
неочевидный фактор, который тоже нужно учитывать. Когда мы прогоняем
алгоритмы через наши данные, должна показываться примерно одна и та же
картинка. Чтобы оценить стабильность, была придумана такая метрика,
как Procrustes distance.    Например, у нас есть исходные данные в
трехмерном пространстве, мы проецируем их, а далее берем, и проецируем
не все наши данные, а только сэмпл, получаем проекцию.    Затем,
пытаемся Y’ замапить в нашу оригинальную визуализацию. Эта проблема
называется Orthogonal Procrustes Problem. Мы используем Procrustes
Transformations: смещение, поворот, равномерное масштабирование. Здесь
необходимо понять, насколько эти проекции разные и насколько они
похожи.    Далее, нужно измерить и рассчитать оптимальную
трансформацию, измерить дистанцию между каждой соответствующей точкой,
и средняя дистанция из всего датасета будет наша Procrustes distance.
На примере ниже очень хорошо видно влияние стабильности на отображении
данных. Один датасет мы визуализируем при помощи UMAP и t-SNE. Если
рассматривать UMAP, визуализация получилась достаточно стабильной,
можно заметить примерно такую же структуру многообразия, примерно
такие же кластеры.    Таким образом, мы выявили некоторые метрики,
которые можно использовать на практике для оценки визуализации: Global
Score, KNN accuracy, Trustworthiness & Discontinuity, IMD.  Материал
проработан совместно с ИТ-библиотекой Innevia
